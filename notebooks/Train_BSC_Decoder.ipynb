{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  multiprocessing  as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import commpy as cp\n",
    "import tensorflow as tf\n",
    "from commpy.channelcoding import Trellis\n",
    "\n",
    "\n",
    "from deepcom.model import NRSCDecoder           # Neural Decoder Model\n",
    "from deepcom.metrics import BER, BLER           # metrics to benchmark Neural Decoder Model\n",
    "from deepcom.utils import corrupt_signal        # simulate a AWGN Channel\n",
    "\n",
    "from deepcom.dataset import create_bsc_dataset  # Create synthetic dataset\n",
    "from deepcom.dataset import data_genenerator    # data loader for Tensorflow\n",
    "\n",
    "import  matplotlib.pyplot  as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training data\n",
    "BLOCK_LEN = 100\n",
    "NUM_TRAINING_DATA = 120000\n",
    "NUM_TESTING_DATA  = 4000\n",
    "\n",
    "# ######################\n",
    "# Network Architectures\n",
    "# ######################\n",
    "NUM_LAYERS = 2\n",
    "NUM_HIDDEN_UNITS = 400\n",
    "\n",
    "# ##############################\n",
    "# Hyper-parameters for training\n",
    "# ##############################\n",
    "BATCH_SIZE = 800       # depends on size of GPU, should be a factor of num_data\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_RATE= 0.5\n",
    "\n",
    "# #######################\n",
    "# Encoder Params\n",
    "# #######################\n",
    "CONSTRAINT_LEN = 3     # num of shifts in Conv. Encoder\n",
    "TRACE_BACK_DEPTH = 15  # (?) a parameter Viterbi Encoder\n",
    "\n",
    "G = np.array([[0o7, 0o5]]) \n",
    "M = np.array([CONSTRAINT_LEN - 1])\n",
    "trellis = Trellis(M, G, feedback=0o7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data....\n",
      "Creating testing data....\n",
      "Number of training sequences 120000\n",
      "Number of testing sequences 4000\n"
     ]
    }
   ],
   "source": [
    "# Ref: Communication Algo via Deep Learning (page 5, last paragraph)\n",
    "Error_Prob = 0.08\n",
    "\n",
    "# ############################\n",
    "# Create dataset \n",
    "# #############################\n",
    "print('Creating training data....')\n",
    "X_train, Y_train = create_bsc_dataset(\n",
    "    NUM_TRAINING_DATA, \n",
    "    BLOCK_LEN, \n",
    "    trellis, \n",
    "    error_prob=Error_Prob, seed=2018)\n",
    "\n",
    "print('Creating testing data....')\n",
    "X_test, Y_test = create_bsc_dataset(\n",
    "    NUM_TESTING_DATA, \n",
    "    BLOCK_LEN, \n",
    "    trellis, \n",
    "    error_prob=Error_Prob,  seed=1111)\n",
    "\n",
    "# X_test, Y_test = X_train, Y_train\n",
    "print('Number of training sequences {}'.format(len(X_train)))\n",
    "print('Number of testing sequences {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Neural Decoder Optimal Performance \n",
    "\n",
    "* Since we learn in advance that **Viterbi is optimal solution for Block Error Rate** (BLER), we can compute what is the global minima (Optimal Solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Optimal Neural Decoder Performance at Error Probability = 0.08 \n",
      "[BER] = 0.038 [BLER] = 0.728 -- 15.040 s\n"
     ]
    }
   ],
   "source": [
    "def run_viterbi(message_bits, noisy_bits):\n",
    "    decoded_bits = cp.channelcoding.viterbi_decode(\n",
    "        coded_bits=noisy_bits.astype(float), \n",
    "        trellis=trellis,\n",
    "        tb_depth=TRACE_BACK_DEPTH,\n",
    "        decoding_type='hard')\n",
    "    num_bit_errors_per_message = cp.utilities.hamming_dist(\n",
    "        message_bits.astype(int),\n",
    "        decoded_bits[:-int(M)].astype(int))\n",
    "    return num_bit_errors_per_message\n",
    "\n",
    "ORIGNAL_BITS = Y_test.reshape((-1, BLOCK_LEN))\n",
    "NOISY_SIGNALS= X_test.reshape((-1, 2 * BLOCK_LEN + 4))\n",
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "try:\n",
    "    error_prob = Error_Prob\n",
    "    print('Estimating Optimal Neural Decoder Performance at Error Probability = %.2f ' % error_prob)\n",
    "    t0 = time.time()\n",
    "    viterbi_hamm_dists = pool.starmap(\n",
    "        func=run_viterbi, \n",
    "        iterable=[(msg_bits, noisy) for msg_bits, noisy in zip(ORIGNAL_BITS,NOISY_SIGNALS)])\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Compute BER and BLER \n",
    "    vi_ber = sum(viterbi_hamm_dists) / np.product(np.shape(Y_test))\n",
    "    vi_bler = np.count_nonzero(viterbi_hamm_dists) / len(Y_test)        \n",
    "    print(\"[BER] = %.3f [BLER] = %.3f -- %3.3f s\" % \n",
    "            (vi_ber, vi_bler, t1 - t0))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline for Neural Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 2)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 800)         967200    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 800)         3200      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 800)         2882400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 800)         3200      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 1)           801       \n",
      "=================================================================\n",
      "Total params: 3,856,801\n",
      "Trainable params: 3,853,601\n",
      "Non-trainable params: 3,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Construct Neural Decoder\n",
    "inputs = tf.keras.Input(shape=(None, 2))\n",
    "outputs = NRSCDecoder(\n",
    "    inputs, \n",
    "    is_training=True, \n",
    "    num_layers=NUM_LAYERS, \n",
    "    hidden_units=NUM_HIDDEN_UNITS, \n",
    "    dropout=DROPOUT_RATE)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    tf.keras.optimizers.SGD(lr=LEARNING_RATE, momentum=0.9, nesterov=True), \n",
    "    'binary_crossentropy', [BER, BLER])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return LEARNING_RATE\n",
    "    elif 10 <= epoch < 15:\n",
    "        return LEARNING_RATE/100.\n",
    "    else:\n",
    "       return LEARNING_RATE / 1000.\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "150/150 [==============================] - 395s 3s/step - loss: 0.3424 - BER: 0.1027 - BLER: 1.0000 - val_loss: 0.6369 - val_BER: 0.1230 - val_BLER: 1.0000\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
      "150/150 [==============================] - 384s 3s/step - loss: 0.2985 - BER: 0.0804 - BLER: 0.9999 - val_loss: 0.5420 - val_BER: 0.0782 - val_BLER: 1.0000\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
      "136/150 [==========================>...] - ETA: 35s - loss: 0.2924 - BER: 0.0801 - BLER: 0.9999"
     ]
    }
   ],
   "source": [
    "# Set up Data Loader using tf.Dataset\n",
    "X_train = X_train[:, :BLOCK_LEN, :]\n",
    "X_test  = X_test[:,  :BLOCK_LEN, :]\n",
    "\n",
    "train_set = data_genenerator(X_train, Y_train, BATCH_SIZE, shuffle=True)\n",
    "test_set = data_genenerator(X_test, Y_test, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Save the best model\n",
    "backup = tf.keras.callbacks.ModelCheckpoint(                     \n",
    "  filepath='BiGRU_BSC.hdf5',\n",
    "  monitor='val_BLER',\n",
    "  save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_set.make_one_shot_iterator(), \n",
    "    steps_per_epoch=len(X_train) //BATCH_SIZE, \n",
    "    validation_data=test_set.make_one_shot_iterator(),\n",
    "    validation_steps= len(X_test) //BATCH_SIZE,\n",
    "    callbacks=[backup, lr_scheduler],\n",
    "    epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of the number of epochs# Count \n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "# Visualize loss history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'r--')\n",
    "plt.plot(epochs, history.history['val_loss'], 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best-fit model\n",
    "model = tf.keras.models.load_model('BiGRU_BSC.hdf5',{'BER': BER, 'BLER': BLER})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test, batch_size=BATCH_SIZE).round()\n",
    "\n",
    "hamming_dists = [cp.utilities.hamming_dist(\n",
    "        x.astype(int),\n",
    "        y.astype(int)\n",
    "    ) for x, y in zip(predictions, Y_test)]\n",
    "\n",
    "                        \n",
    "print('BER: %.4f' % (np.sum(hamming_dists) /  np.product(np.shape(Y_test))))\n",
    "print('BLER: %.4f'% (np.count_nonzero(hamming_dists) / len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
