{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  multiprocessing  as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import commpy as cp\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepcom.model import NRSCDecoder           # Neural Decoder Model\n",
    "from deepcom.metrics import BER, BLER           # metrics to benchmark Neural Decoder Model\n",
    "from deepcom.utils import corrupt_signal        # simulate a AWGN Channel\n",
    "\n",
    "from deepcom.dataset import create_dataset      # Create synthetic dataset\n",
    "from deepcom.dataset import data_genenerator    # data loader for Tensorflow\n",
    "\n",
    "import  matplotlib.pyplot  as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training data\n",
    "BLOCK_LEN = 100\n",
    "NUM_TRAINING_DATA = 120000\n",
    "NUM_TESTING_DATA  = 10000\n",
    "\n",
    "# Communication Algo via Deep Learning\n",
    "#(page 5, last paragraph)\n",
    "NOISE_TYPE ='awgn'\n",
    "SNR_train = 0.0\n",
    "\n",
    "# Network Architectures\n",
    "NUM_LAYERS = 2\n",
    "NUM_HIDDEN_UNITS = 400\n",
    "\n",
    "# Hyper-parameters for training\n",
    "BATCH_SIZE = 500       # depends on size of GPU, should be a factor of num_data\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT_RATE= 0.75\n",
    "\n",
    "\n",
    "CONSTRAINT_LEN = 3     # num of shifts in Conv. Encoder\n",
    "TRACE_BACK_DEPTH = 15  # (?) a parameter Viterbi Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data....\n",
      "Creating testing data....\n",
      "Number of training sequences 120000\n",
      "Number of testing sequences 10000\n"
     ]
    }
   ],
   "source": [
    "from commpy.channelcoding import Trellis\n",
    "\n",
    "#  Generator Matrix (octal representation)\n",
    "G = np.array([[0o7, 0o5]]) \n",
    "M = np.array([CONSTRAINT_LEN - 1])\n",
    "trellis = Trellis(M, G, feedback=0o7, code_type='rsc')\n",
    "\n",
    "# Create dataset \n",
    "print('Creating training data....')\n",
    "X_train, Y_train = create_dataset(\n",
    "    NUM_TRAINING_DATA, \n",
    "    BLOCK_LEN, \n",
    "    trellis, \n",
    "    noise_type=NOISE_TYPE, snr=SNR_train, seed=2018)\n",
    "\n",
    "print('Creating testing data....')\n",
    "X_test, Y_test = create_dataset(\n",
    "    NUM_TESTING_DATA, \n",
    "    BLOCK_LEN, \n",
    "    trellis, \n",
    "    noise_type=NOISE_TYPE, snr=SNR_train, seed=1111)\n",
    "\n",
    "print('Number of training sequences {}'.format(len(X_train)))\n",
    "print('Number of testing sequences {}'.format(len(Y_test)))\n",
    "# print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 2)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 800)         967200    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 800)         3200      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 800)         2882400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 800)         3200      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 1)           801       \n",
      "=================================================================\n",
      "Total params: 3,856,801\n",
      "Trainable params: 3,853,601\n",
      "Non-trainable params: 3,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Construct Neural Decoder\n",
    "inputs = tf.keras.Input(shape=(None, 2))\n",
    "outputs = NRSCDecoder(\n",
    "    inputs, \n",
    "    is_training=True, \n",
    "    num_layers=NUM_LAYERS, \n",
    "    hidden_units=NUM_HIDDEN_UNITS, \n",
    "    dropout=DROPOUT_RATE)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Set up training \n",
    "model.compile('adam', 'binary_crossentropy', [BER])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 32/240 [===>..........................] - ETA: 5:12 - loss: 0.5102 - BER: 0.1906"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-41845c77eed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     epochs=100)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Load best-fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/deepcom/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda2/envs/deepcom/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m           logging.warning('Your dataset iterator ran out of data; '\n",
      "\u001b[0;32m~/miniconda2/envs/deepcom/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/deepcom/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up Data Loader using tf.Dataset\n",
    "train_set = data_genenerator(X_train, Y_train, BATCH_SIZE, shuffle=True)\n",
    "test_set = data_genenerator(X_test, Y_test, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Backup best model\n",
    "backup = tf.keras.callbacks.ModelCheckpoint(                     \n",
    "  filepath='BiGRU.hdf5',\n",
    "  monitor='val_loss',\n",
    "  save_best_only=True)\n",
    "    \n",
    "# Stop training early if the model seems to overfit\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0,\n",
    "    patience=3,\n",
    "    verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    train_set.make_one_shot_iterator(), \n",
    "    steps_per_epoch=len(X_train) //BATCH_SIZE, \n",
    "    validation_data=test_set.make_one_shot_iterator(),\n",
    "    validation_steps= len(X_test) //BATCH_SIZE,\n",
    "    callbacks=[early_stopping, backup],\n",
    "    epochs=100)\n",
    "\n",
    "# Load best-fit model\n",
    "model = tf.keras.models.load_model('BiGRU.hdf5',{'BER': BER})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of the number of epochs# Count \n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "# Visualize loss history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'r--')\n",
    "plt.plot(epochs, history.history['val_loss'], 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
