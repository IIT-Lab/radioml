{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  multiprocessing  as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import commpy as cp\n",
    "import tensorflow as tf\n",
    "from commpy.channelcoding import Trellis\n",
    "\n",
    "\n",
    "from deepcom.model import NRSCDecoder           # Neural Decoder Model\n",
    "from deepcom.metrics import BER, BLER           # metrics to benchmark Neural Decoder Model\n",
    "from deepcom.utils import corrupt_signal        # simulate a AWGN Channel\n",
    "\n",
    "from deepcom.dataset import create_dataset      # Create synthetic dataset\n",
    "from deepcom.dataset import data_genenerator    # data loader for Tensorflow\n",
    "\n",
    "import  matplotlib.pyplot  as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training data\n",
    "BLOCK_LEN = 100\n",
    "NUM_TRAINING_DATA = 120000\n",
    "NUM_TESTING_DATA  = 20000\n",
    "\n",
    "# ######################\n",
    "# Network Architectures\n",
    "# ######################\n",
    "NUM_LAYERS = 2\n",
    "NUM_HIDDEN_UNITS = 400\n",
    "\n",
    "# ##############################\n",
    "# Hyper-parameters for training\n",
    "# ##############################\n",
    "BATCH_SIZE = 200       # depends on size of GPU, should be a factor of num_data\n",
    "LEARNING_RATE = 0.0004\n",
    "DROPOUT_RATE= 0.7\n",
    "\n",
    "# #######################\n",
    "# Encoder Params\n",
    "# #######################\n",
    "CONSTRAINT_LEN = 3     # num of shifts in Conv. Encoder\n",
    "TRACE_BACK_DEPTH = 15  # (?) a parameter Viterbi Encoder\n",
    "\n",
    "G = np.array([[0o7, 0o5]]) \n",
    "M = np.array([CONSTRAINT_LEN - 1])\n",
    "trellis = Trellis(M, G, feedback=0o7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data....\n",
      "Creating testing data....\n",
      "Number of training sequences 120000\n",
      "Number of testing sequences 20000\n"
     ]
    }
   ],
   "source": [
    "# Ref: Communication Algo via Deep Learning (page 5, last paragraph)\n",
    "SNR_train = 1.0\n",
    "\n",
    "# ############################\n",
    "# Create dataset \n",
    "# #############################\n",
    "print('Creating training data....')\n",
    "X_train, Y_train = create_dataset(\n",
    "    NUM_TRAINING_DATA, \n",
    "    BLOCK_LEN, \n",
    "    trellis, \n",
    "    snr=SNR_train, seed=2018, \n",
    "    num_cpus=mp.cpu_count())\n",
    "\n",
    "print('Creating testing data....')\n",
    "X_test, Y_test = create_dataset(\n",
    "    NUM_TESTING_DATA, \n",
    "    BLOCK_LEN, \n",
    "    trellis, \n",
    "    snr=SNR_train, seed=1111, \n",
    "    num_cpus=mp.cpu_count())\n",
    "\n",
    "print('Number of training sequences {}'.format(len(X_train)))\n",
    "print('Number of testing sequences {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Neural Decoder Optimal Performance \n",
    "\n",
    "* Since we learn in advance that **Viterbi is optimal solution for Block Error Rate** (BLER), we can compute what is the global minima (Optimal Solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Optimal Neural Decoder Performance at SNR = 1.00 \n",
      "[BER] = 0.046 [BLER] = 0.764 -- 84.196 s\n"
     ]
    }
   ],
   "source": [
    "def run_viterbi(message_bits, noisy_bits):\n",
    "    # Viterbi Decoder on Conv. Code\n",
    "    decoded_bits = cp.channelcoding.viterbi_decode(\n",
    "        coded_bits=noisy_bits, \n",
    "        trellis=trellis,\n",
    "        tb_depth=TRACE_BACK_DEPTH,\n",
    "        decoding_type='unquantized')\n",
    "    num_bit_errors_per_message = cp.utilities.hamming_dist(\n",
    "        message_bits.astype(int),\n",
    "        decoded_bits[:-int(M)].astype(int))\n",
    "    return num_bit_errors_per_message\n",
    "\n",
    "ORIGNAL_BITS = Y_test.reshape((-1, BLOCK_LEN))\n",
    "NOISY_SIGNALS= X_test.reshape((-1, 2 * BLOCK_LEN + 4))\n",
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "try:\n",
    "    snr = SNR_train\n",
    "    print('Estimating Optimal Neural Decoder Performance at SNR = %.2f ' % snr)\n",
    "    t0 = time.time()\n",
    "    viterbi_hamm_dists = pool.starmap(\n",
    "        func=run_viterbi, \n",
    "        iterable=[(msg_bits, noisy) for msg_bits, noisy in zip(ORIGNAL_BITS,NOISY_SIGNALS)])\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Compute BER and BLER \n",
    "    vi_ber = sum(viterbi_hamm_dists) / np.product(np.shape(Y_test))\n",
    "    vi_bler = np.count_nonzero(viterbi_hamm_dists) / len(Y_test)        \n",
    "    print(\"[BER] = %.3f [BLER] = %.3f -- %3.3f s\" % \n",
    "            (vi_ber, vi_bler, t1 - t0))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline for Neural Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 2)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 800)         967200    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 800)         3200      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 800)         2882400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 800)         3200      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 1)           801       \n",
      "=================================================================\n",
      "Total params: 3,856,801\n",
      "Trainable params: 3,853,601\n",
      "Non-trainable params: 3,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Construct Neural Decoder\n",
    "inputs = tf.keras.Input(shape=(None, 2))\n",
    "outputs = NRSCDecoder(\n",
    "    inputs, \n",
    "    is_training=True, \n",
    "    num_layers=NUM_LAYERS, \n",
    "    hidden_units=NUM_HIDDEN_UNITS, \n",
    "    dropout=DROPOUT_RATE)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 279s 465ms/step - loss: 0.2599 - BER: 0.1077 - BLER: 0.9996 - val_loss: 0.1973 - val_BER: 0.0800 - val_BLER: 0.9988\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 266s 443ms/step - loss: 0.1947 - BER: 0.0787 - BLER: 0.9958 - val_loss: 0.1716 - val_BER: 0.0684 - val_BLER: 0.9920\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 270s 451ms/step - loss: 0.1788 - BER: 0.0719 - BLER: 0.9910 - val_loss: 0.1633 - val_BER: 0.0645 - val_BLER: 0.9820\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 277s 461ms/step - loss: 0.1683 - BER: 0.0675 - BLER: 0.9856 - val_loss: 0.1572 - val_BER: 0.0621 - val_BLER: 0.9776\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 284s 473ms/step - loss: 0.1605 - BER: 0.0643 - BLER: 0.9792 - val_loss: 0.1522 - val_BER: 0.0602 - val_BLER: 0.9708\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 281s 468ms/step - loss: 0.1549 - BER: 0.0619 - BLER: 0.9746 - val_loss: 0.1478 - val_BER: 0.0584 - val_BLER: 0.9632\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 297s 496ms/step - loss: 0.1504 - BER: 0.0600 - BLER: 0.9687 - val_loss: 0.1445 - val_BER: 0.0566 - val_BLER: 0.9552\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 317s 528ms/step - loss: 0.1459 - BER: 0.0581 - BLER: 0.9598 - val_loss: 0.1394 - val_BER: 0.0549 - val_BLER: 0.9464\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 324s 541ms/step - loss: 0.1415 - BER: 0.0562 - BLER: 0.9510 - val_loss: 0.1362 - val_BER: 0.0536 - val_BLER: 0.9316\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 323s 539ms/step - loss: 0.1379 - BER: 0.0546 - BLER: 0.9411 - val_loss: 0.1428 - val_BER: 0.0560 - val_BLER: 0.9476\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "600/600 [==============================] - 321s 535ms/step - loss: 0.1354 - BER: 0.0536 - BLER: 0.9337 - val_loss: 0.1296 - val_BER: 0.0505 - val_BLER: 0.9028\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 4e-05.\n",
      "600/600 [==============================] - 325s 541ms/step - loss: 0.1324 - BER: 0.0524 - BLER: 0.9258 - val_loss: 0.1282 - val_BER: 0.0504 - val_BLER: 0.9004\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 4e-05.\n",
      "600/600 [==============================] - 324s 540ms/step - loss: 0.1308 - BER: 0.0517 - BLER: 0.9222 - val_loss: 0.1277 - val_BER: 0.0502 - val_BLER: 0.8992\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 4e-05.\n",
      "600/600 [==============================] - 325s 541ms/step - loss: 0.1300 - BER: 0.0513 - BLER: 0.9210 - val_loss: 0.1274 - val_BER: 0.0499 - val_BLER: 0.8972\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 4e-05.\n",
      "600/600 [==============================] - 325s 542ms/step - loss: 0.1293 - BER: 0.0511 - BLER: 0.9194 - val_loss: 0.1268 - val_BER: 0.0497 - val_BLER: 0.8920\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 4e-05.\n",
      "600/600 [==============================] - 325s 542ms/step - loss: 0.1288 - BER: 0.0509 - BLER: 0.9185 - val_loss: 0.1270 - val_BER: 0.0497 - val_BLER: 0.8940\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 4e-06.\n",
      "600/600 [==============================] - 326s 544ms/step - loss: 0.1284 - BER: 0.0508 - BLER: 0.9159 - val_loss: 0.1266 - val_BER: 0.0496 - val_BLER: 0.8928\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 4e-06.\n",
      "600/600 [==============================] - 327s 545ms/step - loss: 0.1283 - BER: 0.0507 - BLER: 0.9164 - val_loss: 0.1265 - val_BER: 0.0495 - val_BLER: 0.8936\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 4e-06.\n",
      " 49/600 [=>............................] - ETA: 4:50 - loss: 0.1306 - BER: 0.0515 - BLER: 0.9160"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d963307747fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     epochs=20)\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/deepcom/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda2/envs/deepcom/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m           logging.warning('Your dataset iterator ran out of data; '\n",
      "\u001b[0;32m~/miniconda2/envs/deepcom/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/deepcom/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up Data Loader using tf.Dataset\n",
    "X_train = X_train[:, :BLOCK_LEN, :]\n",
    "X_test  = X_test[:,  :BLOCK_LEN, :]\n",
    "\n",
    "train_set = data_genenerator(X_train, Y_train, BATCH_SIZE, shuffle=True)\n",
    "test_set = data_genenerator(X_test, Y_test, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Save the best model\n",
    "backup = tf.keras.callbacks.ModelCheckpoint(                     \n",
    "  filepath='BiGRU.hdf5',\n",
    "  monitor='val_BLER',\n",
    "  save_best_only=True)\n",
    "    \n",
    "# Stop training early if the model seems to overfit\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    verbose=0, mode='auto')\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch):\n",
    "  if epoch > 10 and epoch <=15:\n",
    "      lr = LEARNING_RATE/10.0\n",
    "  elif epoch >15 and epoch <=20:\n",
    "      lr = LEARNING_RATE/100.0\n",
    "  elif epoch >20 and epoch <=25:\n",
    "      lr =LEARNING_RATE/1000.0\n",
    "  elif epoch > 25:\n",
    "      lr = LEARNING_RATE/10000.0\n",
    "  else:\n",
    "      lr =LEARNING_RATE\n",
    "  return lr\n",
    "  \n",
    "change_lr = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "model.compile(\n",
    "    tf.keras.optimizers.Adam(LEARNING_RATE), \n",
    "    'binary_crossentropy', [BER, BLER])\n",
    "\n",
    "history = model.fit(\n",
    "    train_set.make_one_shot_iterator(), \n",
    "    steps_per_epoch=len(X_train) //BATCH_SIZE, \n",
    "    validation_data=test_set.make_one_shot_iterator(),\n",
    "    validation_steps= len(X_test) //BATCH_SIZE,\n",
    "    callbacks=[early_stopping, backup, change_lr],\n",
    "    epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a89b31964177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Count of the number of epochs# Count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Visualize loss history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Count of the number of epochs# Count \n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "# Visualize loss history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'r--')\n",
    "plt.plot(epochs, history.history['val_loss'], 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best-fit model\n",
    "model = tf.keras.models.load_model('BiGRU.hdf5',{'BER': BER, 'BLER': BLER})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BER: 0.0497\n",
      "BLER: 0.8920\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test, batch_size=BATCH_SIZE).round()\n",
    "\n",
    "hamming_dists = [cp.utilities.hamming_dist(\n",
    "        x.astype(int),\n",
    "        y.astype(int)\n",
    "    ) for x, y in zip(predictions, Y_test)]\n",
    "\n",
    "                        \n",
    "print('BER: %.4f' % (np.sum(hamming_dists) /  np.product(np.shape(Y_test))))\n",
    "print('BLER: %.4f'% (np.count_nonzero(hamming_dists) / len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
